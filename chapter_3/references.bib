@article{vlux07,
  title={A tutorial on spectral clustering},
  author={Von Luxburg, U.},
  journal={Statistics and computing},
  pages={395--416},
  year={2007}
}

@inproceedings{che69,
  title={A lower bound for the smallest eigenvalue of the Laplacian},
  author={Cheeger, J.},
  booktitle={Proceedings of the Princeton conference in honor of Professor S. Bochner},
  pages={195--199},
  year={1969}
}

@inproceedings{bsh10,
	author = {Bshouty, Nader H. and Long, Philip M.},
	title = {Finding Planted Partitions in Nearly Linear Time Using Arrested Spectral Clustering},
	year = {2010},
	isbn = {9781605589077},
	publisher = {Omnipress},
	address = {Madison, WI, USA},
	booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
	pages = {135–142},
	numpages = {8},
	location = {Haifa, Israel},
	series = {ICML’10}
}

@InProceedings{chau12,
  title = 	 {Spectral Clustering of Graphs with General Degrees in the Extended Planted Partition Model},
  author = 	 {Kamalika Chaudhuri and Fan Chung and Alexander Tsiatas},
  booktitle = 	 {Proceedings of the 25th Annual Conference on Learning Theory},
  pages = 	 {35.1--35.23},
  year = 	 {2012},
  editor = 	 {Shie Mannor and Nathan Srebro and Robert C. Williamson},
  volume = 	 {23},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Edinburgh, Scotland},
  month = 	 {25--27 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v23/chaudhuri12/chaudhuri12.pdf},
  url = 	 {http://proceedings.mlr.press/v23/chaudhuri12.html},
  abstract = 	 {In this paper, we examine a spectral clustering algorithm for similarity graphs drawn from a simple random graph model, where nodes are allowed to have varying degrees, and we provide theoretical bounds on its performance. The random graph model we study is the Extended Planted Partition (EPP) model, a variant of the classical planted partition model. The standard approach to spectral clustering of graphs is to compute the bottom \emphk singular vectors or eigenvectors of a suitable graph Laplacian, project the nodes of the graph onto these vectors, and then use an iterative clustering algorithm on the projected nodes. However a challenge with applying this approach to graphs generated from the EPP model is that unnormalized Laplacians do not work, and normalized Laplacians do not concentrate well when the graph has a number of low degree nodes. We resolve this issue by introducing the notion of a degree-corrected graph Laplacian. For graphs with many low degree nodes, degree correction has a regularizing effect on the Laplacian. Our spectral clustering algorithm projects the nodes in the graph onto the bottom \emphk right singular vectors of the degree-corrected random-walk Laplacian, and clusters the nodes in this subspace. We show guarantees on the performance of this algorithm, demonstrating that it outputs the correct partition under a wide range of parameter values. Unlike some previous work, our algorithm does not require access to any generative parameters of the model.}
}

@InProceedings{vlux04,
	author="von Luxburg, Ulrike
	and Bousquet, Olivier
	and Belkin, Mikhail",
	editor="Shawe-Taylor, John
	and Singer, Yoram",
	title="On the Convergence of Spectral Clustering on Random Samples: The Normalized Case",
	booktitle="Learning Theory",
	year="2004",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="457--471",
	abstract="Given a set of n randomly drawn sample points, spectral clustering in its simplest form uses the second eigenvector of the graph Laplacian matrix, constructed on the similarity graph between the sample points, to obtain a partition of the sample. We are interested in the question how spectral clustering behaves for growing sample size n. In case one uses the normalized graph Laplacian, we show that spectral clustering usually converges to an intuitively appealing limit partition of the data space. We argue that in case of the unnormalized graph Laplacian, equally strong convergence results are difficult to obtain.",
	isbn="978-3-540-27819-1"
}


