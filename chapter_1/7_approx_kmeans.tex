% Contributers: Noah Huber-Feely, Samuel Sharpe
\section{(1+$\epsilon$)-approximate $k$-means clustering}

Unlike Lloyd's method, this approximation algorithm guarantees that it will have
a cost no more than (1+$\epsilon$)-times the true minimum cost and will do so in
in time O($n \log^k n$) for any $\epsilon>0$. This method uses intelligent
preprocessing of the space to achieve this result. Formally, we define
$\Pi = (S_1, S_2, ..., S_k)$ to be some partitioning of an $n$-point set
$X \subset \R^d$ for some number $k \geq 2$. Further, cost($\Pi$)=$\sum_{i=1}^k \text{cost(}S_i)$ where cost($S$)=
$\sum_{x \in S} \norm{x-c(S)}^2$. Here $c(s)=(1/|S|) \sum_{x \in S} x$ and is
thus the centroid of the set $S$. Let diam($X$) denote the diameter of a set
$X \subset \R^d$. Further, this and other centroid base clustering methods relies upon the Vornoi partition of a set of points by the centroid points.
For centroids $c_1,...,c_k$, we refer to the Vornoi partition $(S_1,S_2,...,S_k)$ of a set
as $\Pi_{Vor}(c_1, c_2, \dots, c_k))$.

We say that a $k$-clustering $\Pi$ of $X$ is (1 + $\epsilon$)-approximately optimal if cost($\Pi$) $\leq$ (1 + $\epsilon$)cost($\Pi'$) for any $k$-clustering $\Pi'$ of $X$. The method described below thus achieves the results outlined in the following two theorems.

\begin{theorem}
Let $X \subset \R^d$ be an n-point set, and let $\epsilon >0$ be given. A (1 + $\epsilon$)-approximately optimal 2-clustering of X can be found in time $O(n \log n \cdot \epsilon^{-2d} \log \frac{1}{\epsilon} + n \epsilon^{-(4d-2))} \log \frac{1}{\epsilon})$ so the running time is $O(n \log n)$ for a fixed $\epsilon>0$.
\end{theorem}

\begin{theorem}
Let $X \subset \R^d$ be an n-point set, let $k \geq 3$ be fixed and let $\epsilon >0$ be given. A (1 + $\epsilon$)-approximately optimal k-clustering of X can be found in time $O(n(\log n)^k \epsilon^{-2k^2d})$.
\end{theorem}

We thus see that we achieve a near-linear time algorithm for both (1 + $\epsilon$)-approximate 2-means and k-means.

However, for all the technical guarantees of this approach, it is not often used in practice
with heuristic algorithms typically used instead of it. One primary reason is that,
despite having $O(n \log n)$ runtime, for a constant factor $\epsilon$, the multiplicative
constant is exponential in the number of dimension. Consider a case where
$\epsilon=0.25$ and $d=5$ dimensions. The multiplicative constant
will thus be $0.25^{-2 \cdot 5} \log (1/0.25) \approx 1,500,000$. This gets
exponentially worse as dimensions increase or more precision is desired. Thus,
as in practice heuristic algorithms typically generate reasonable approximations
this runtime cost negates the practical benefits of the guaranteed approximate
optimality.

\subsection{Preliminaries and definitions}

This approach requires some additional preliminary terminology and cost definitions which are themselves interesting ideas for this problem space.
\begin{definition}[$\epsilon$-near]
  Let $\epsilon \geq 0$. We define $\sim_{\epsilon}$ to be a relation on ordered pairs of points in $\R^d$. We say that $(x,y)$ and $(x',y')$ are $\epsilon$-near if $(x,y) \sim_{\epsilon} (x', y')$ such that  $(x,y) \sim_{\epsilon} (x', y')$ if $\norm{x-x'} \leq \epsilon \cdot \norm{x-y}$ and $\norm{y - y'} \leq \epsilon \cdot \norm{x - y}$.
\end{definition}

\begin{definition}[$\epsilon$-separated]
Let $P$ be a set of ordered pairs of points of $\R^d$. We say $P$ is $\epsilon$-separated if no two pairs in $P$ are $\epsilon$-near.
\end{definition}

\noindent \emph{Approximate range searching}. An important and useful concept for optimizing this problem is approximate range searching. At a high level, this method uses intelligent preprocessing to attempt to find an approximate answer to the sum of element weights within a specific range for a set $P \subset \R^d$ where each point is assigned a weight such that for $p \in P$, $w(p) \in S$ where $S$ is a commmutative semigroup (meaning weights can be summed). Formally, given a range $R \in \cR$ where $\cR$ is a class of admissible ranges and some $\epsilon>0$, we define $R^+$ to be the set of all points at most a distance of $\epsilon \cdot$diam($R$) from $R$, and let $R^-$ be all points of distance at most $\epsilon \cdot$diam($R$) from the complement of $R$.

\begin{definition}[$\epsilon$-approximate intersection]
Let $P$ be a an $n$ point subset of $\R^d$ and let $R$ be a range. Then an $\epsilon$-aproximate intersection of P with a range R is any subset $P_1 \subseteq P$ with $P \cap R^- \subseteq P_1 \subset P \subseteq R^+$.
\end{definition}

\begin{definition}[$\epsilon$-approximate answer]
An $\epsilon$-approximate answer to the query with range $R$ is any weight of the form $\sum_{p \in P_1} w(p)$, where $P_1$ is any $\epsilon$-aproximate intersection of $P$ with $R$.
\end{definition}

This method takes $O(n \log n)$ preprocessing time and can then produce an $\epsilon$-approximate answer in $O( \log n + \epsilon^{-d})$ as shown by Arya and Mount.

\noindent \emph{Putting the points on a polynomial-size grid}. The proposed k-clustering problem works with points with polynomially large distances between them, however ratios of interpoint distances can be arbitrarily large. The set of points $X$ can be `snapped' onto a integer grid with polynomial size (specifically $O(n^3/\epsilon)$). If there is some algorithm that can compute an $(1+\epsilon)$-approximate clustering for $X'$. Then a $(1+\epsilon)$-approximate clustering for the arbitrarily large $X$ can be computed with $O(n \log n)$ processing time. This is tangential to the main result so we do not present the proof here. Now we proceed assuming the n-point set $X \subset \R^d$ lie on an integer grid of size $O(n^3/\epsilon)$.\\

\noindent \textbf{Approximate Centroid Sets}\\
Let $S$ be a finite set in $\R^d$. The quadratic-mean radius of $S$ is
\begin{align}
    \rho(S)  = \left( \frac{1}{|S|}\sum_{x\in S}\norm{x-c(S)}^2\right)^{1/2}
    = \sqrt{\frac{cost(S)}{\abs{S}}}
\end{align} 
be the \emph{quadratic-mean radius} of $S$. For a real number $\epsilon \geq 0$, the \emph{$\epsilon$-tolerance ball} of S is the ball centered as $c(S)$ of radius $(\epsilon/3)\rho(S)$.
\begin{definition}[$\epsilon$-approximate centroid set]
Let $X \subset \R^d$ and $C \subset \R^d$ be finite point sets. We call C an $\epsilon$-approximate centroid set for $X$ if $C$ intersects of the $\epsilon$-tolerance ball of each nonempty $S \subseteq X$. If it intersects the $\epsilon$-tolerance ball  of each cluster size s or larger it is a $\epsilon$-approximate centroid set for X for cluster size $\geq s$.
\end{definition}

\begin{lemma}
Let $X \subset \R^d$ be a finite point set, let $k\geq 2$, and let $C$ be an $\epsilon$-approximate centroid set for $X$ for cluster size $\geq s$. Then there are $c_1, c_2, \dots, c_k \in C$ such that 
$$cost(\Pi_{Vor}(c_1, c_2, \dots, c_k)) \leq (1+\epsilon)cost(\Pi)$$
for any k-clustering $\Pi$ of $X$ with all clusters of size at least s. 
\end{lemma}
\begin{proof}
Let $(S_1, S_2, \dots, S_k)$ be an optimal $k$-clustering of $X$ with all clusters of size at least $s$. For $i = 1,2,\dots,k$ choose $c_i \in C$ lying in the $\epsilon$-tolerance ball of cluster $S_i$. We can bound the cost of each cluster $S_i$ given $c_i$ as the center:

\begin{align*}
    cost(S_i, c_i) & = \sum_{x \in S_i}\norm{x-c_i}^2 \leq \sum_{x \in S_i} (\norm{x-c(S_i)} + \norm{c(S_i)-x})^2  && \text{by triangle ineq} \\
    & = cost(S_i) + 2\norm{c_i-c(S_i)}\sum_{x \in S_i}\norm{x-c(S_i)} + \abs{S_i}\norm{c_i-c(S_i)}^2 \\
    & \leq cost(S_i) + 2\frac{\epsilon}{3}\rho(S_i)\sqrt{\abs{S_i}}\sqrt{cost(S_i)} + \abs{S_i}\left(\frac{\epsilon}{3}\rho(S_i) \right)^2 && \text{by cauchy-schwartz } \\
    & \leq cost(S_i) + \frac{2}{3}\epsilon cost(S_i) + \frac{\epsilon^2}{9}cost(S_i) \\
    & \leq (1+\epsilon) cost(S_i)
\end{align*}
Cauchy-Schwartz is applied above as
$$\left( \sum_{x \in S_i}1\norm{x-c(S_i)} \right)^2 \leq \abs{S_i}\sum_{x \in S_i}\norm{x-c(S_i)}^2 $$
$$ \sum_{x \in S_i}\norm{x-c(S_i)} \leq \sqrt{\abs{S_i}cost(S_i)} $$
Finally, if we choose $S_i'$ based on the Voronoi partition of $c_i$
where $(S_1', S_2', \dots S_k') =\Pi_{Vor}(c_1, c_2, \dots, c_k) $, by optimality of the Voronoi partition we have
$$\sum_{i=1}^k cost(S_i') \leq \sum_{i=1}^k cost(S_i', c_i) \leq \sum_{i=1}^k cost(S_i, c_i) \leq (1+\epsilon) \sum_{i=1}^k cost(S_i)$$
\end{proof}

\noindent \emph{Constructing $\epsilon$-approximate centroid set for $X$}. Now we construct an $\epsilon$-approximate centroid set, $C$ for $X$ with a polynomial number of points from which we can identify centroids for the (1+$\epsilon$)-approximate $k$-clustering. Given $X$ (on a integer grid of size $O(n^4)$), minimum cluster size $s$, $\epsilon >0$ and $\delta >0$ where $\delta$ is a lower bound on $\min\norm{x-y}$ for $x,y\in X$. We set $r = \delta/n$ though $\delta/\sqrt{2}$ is sufficient. For any cluster $S \subseteq X$ with at least two distinct points, $\rho(S)\geq r$. 
 $$\rho(S) = \sqrt{\frac{cost(S)}{\abs{S}}} = \sqrt{\frac{1}{2\abs{S}^2}\sum_{x,y \in X}\norm{x-y}^2} \geq \sqrt{\frac{1}{2\abs{S}^2}\sum_{x,y \in X}\delta^2} =  \delta/\sqrt{2}$$
 
To construct this set we will reference \emph{cubes} which are axis-parallel cubes in $\R^d$ and the $K$-enlargement of a cube is a concentric cube with a side $K$-times larger. We let $Q_0$ be the 3-enlargement of the smallest cube enclosing $X$, and $R$ be the side of $Q_0$. We define an \emph{active cube} as any cube containing at least $x/2^{d+1}$ points of X. We initially set $C = \emptyset$. 

\noindent The construction of C begins at each step by repeated subdivision of $Q$ into $2^d$ equal-sized axis aligned cubes. For any cube $Q$ that is active, we let $\sigma$ be the side length of $Q$ and choose $C_Q$ that is $(\frac{\epsilon }{18} \sigma)$-dense for the 2-enlargement of $Q$ (see Figure \ref{fig:cq}). We can say $|C_Q| = O(\epsilon^{-d})$. Finally, we add $C_Q$, to the constructed set C.  After this is done for $Q$, it is no longer active and is further subdivided as long as $\sigma \geq 2r$. We stop when there are no active cubes left. 

\begin{figure}
    \centering
    \includegraphics[width=10cm]{chapter_1/files/Cq.png}
\centering
    \caption{(a) The set $C_Q$ and (b) the subdivision of $Q$ \cite{matousek}}
    \label{fig:cq}
\end{figure}

\begin{lemma}We have $\abs{C} = O((n/s)\epsilon^{-d} \log(nR/\delta))$ and the construction can be formed in time $O((n+(n/s)\epsilon^{-d}) \log(nR/\delta))$. The constructed set C is an $\epsilon$-approximate centroid set for X for cluster size $\geq s$.
\end{lemma}
\begin{proof}
Since the minimum allowable size cube side length is $2r$ and we divide cubes by two in each dimension, we encounter active cubes with at most $O(\log(R/r))$ distinct side lengths. Since there is a lower bound of $s/2^{d+1}$ points in each cube cubes are disjoint, the upperbound on active cubes is $O((n/s)\log(nR/\delta))$. Finally, by the constructed density of $C_Q$ for each active cube the bound on size of C follows. 

\noindent To show that C is an $\epsilon$-approximate centroid set for X we show that $C$ intersects some of these active cubes and is sufficiently close to points in all clusters. We let $S \subseteq X$ be a cluster of size $\geq s$ with at least two points. By Markov's inequality, the ball B or radius $\sqrt{2}\rho(S)$ centered at $c(S)$ contains at least $s/2$ points of $S$. The statement follows from the probability that a given point $x$ is less than $\sqrt{2}\rho(S)$ away from a center:
\begin{align*}
    P(\norm{x-c(S)}^2 \geq 2\rho(S)^2) & \leq \frac{\frac{1}{|S|}\sum_{x\in S} \norm{x-c(S)}^2 }{2\rho(S)^2} 
\end{align*}
Let $j \geq 0$ be the integer sutch that $\sigma = R/2^j \in [3\rho(S),6\rho(S)]$. Since $diam(B) < \sigma$, B intersects at most $2^d$ cubes of side $\sigma$, so there must be a cube $Q$ that contains at least $s/2^{d+1}$ points of X and was active. The point $c(S)$ is at distance at most $\sqrt{2}\rho(S) \leq \sigma/2$ from $Q$, so it lies in the 2-enlargement of Q. Thus, $C_Q \subseteq C$ contains a point at distance at most $\frac{\epsilon}{18} \sigma \leq \frac{\epsilon}{3}\rho(S)$ from $c(S)$. Thus, C intersects the $\epsilon$-tolerance ball of S. 
\end{proof}

\begin{remark}
The authors provide a tighter bound on the size of C to $O(n\epsilon^{-d}\log(1/\epsilon)$ and generalize the finding for any size cluster.
\end{remark}
\newpage
\subsection{$(1+\epsilon)$-approximate 2-means clustering}
We will first address the easier subproblem of developing an $O(n \log n)$
(1+$\epsilon$) approximate 2-clustering. At a high-level, the algorithm uses
the concepts of $\epsilon$-complete sets explored above to generate a set of
pairs of points of $C$ such that at least one of the pairs has a cost that
is (1+$\epsilon$) of the optimal cost. The algorithm then need only loop through
each of the pairs and then intelligently compute the approximate cost (using the
approximate range searching methodology) of using the pair as centroids. The
algorithm then ultimately returns the pair with the lowest approximate cost.
We provide the technical pseudocode below.

\begin{algorithm}[H]
  \caption{(1+$\epsilon$) $2$-means algorithm:}
\begin{algorithmic} 
\STATE Compute an $\epsilon$-approximate centroid set C for X
\STATE Generate the set P of all pairs $(c_1, c_2)$ of distinct points of C
\FORALL {($c_1,c_2$) $\in$ $P$}
    \STATE Compute the Voronoi diagram and produce the hyperplane $h$ bisecting the segment $c_1 c_2$
    \STATE Compute the cost of the 2-clustering given by $h$
\ENDFOR
\STATE Return the 2-clustering with the smallest cost as determined above
\end{algorithmic}
\end{algorithm}

To get an $\epsilon$-approximate answer for an optimal 2-clustering, we don't need to inspect all pairs of distinct points of C.  An $\epsilon$-complete set (for ever pair in $X$ there is an $\epsilon$ near pair in the set) of pairs is sufficient for a $(1+\epsilon)$-approximation. 

\begin{lemma}
Let $c1, c2 \in \R^d$ be points, and let $c_1', c_2'$ be such that the pairs $(c_1,c_2)$ and $(c_1',c_2')$ are $\epsilon$-near, $\epsilon \leq \frac{1}{9}$. Let $\Pi = \Pi_{Vor}(c_1, c_2)$ be the clustering of $X$ by the bisecting hyperplane of $c_1$ and $c_2$, and let $\Pi' = \Pi_{Vor}(c_1', c_2')$. Then $cost(\Pi')\leq (1+ 6 \epsilon)cost(\Pi)$
\end{lemma}

\begin{proof}
Let $\Pi = (S_1, S_2)$ and $\Pi' = (S_1', S_2')$. Given the optimality of Voronoi
  clusters for given centroids, we need to show that the cost of a clustering
  given $\epsilon$-near centroids is $\epsilon$-approximate, i.e. $$cost(S_1', c_1)
  + cost(S_2' , c_2) \leq (1 + 6\epsilon)[cost(S_1, c_1) + cost(S_2 , c_2)]$$ This
  is the same as showing for each $x \in S_1' \backslash S_1$ we have $\norm{x-c_1}
  \leq (1+16\epsilon)\norm{x-c_2}$ and symmetric argument for $x \in S_2' \backslash S_2$.
  Since $x \in S_1'$, x must be closer to $c_1'$ than $c_2'$, $\norm{x-c_1'}\leq  \norm{x-c_2'}$.
  Let $\delta = \norm{c_1'-c_2'}$. By the triangle inequality and definition of $\epsilon$-near we have
\begin{align*}
    \norm{x-c_1} &  \leq \norm{x-c_1'} + \norm{c_1-c_1'} \\
    & \leq \norm{x-c_1'} + \epsilon \norm{c_1'-c_2'}\\
    & = \norm{x-c_1'} + \epsilon \delta \\
    & \leq \norm{x-c_2'}+ \epsilon \delta \\
    & \leq \norm{x-c_2} + \norm{c_2-c_2'} + \epsilon \delta \\
    & \leq \norm{x-c_2} + 2\epsilon \delta
\end{align*}
we also have by similar reasoning
$$\norm{x-c_2}\leq \norm{x-c_2'} - \epsilon \delta \leq \delta/2 - \epsilon \delta$$
which implies $\delta \leq \frac{2}{1-2\epsilon}\norm{x-c_2}\leq 3 \norm{x-c_2}$. Using the bound on $\norm{x-c_2}$ and $\delta$, we have $\norm{x-c_1} \leq (1+ 6\epsilon)\norm{x-c_2}$ as required. 
\end{proof}

It remains to show that this algorithm can be performed in near-linear time.
In a direct implementation of this algorithm, we must consider $|C|^2$ pairs
$(c_1,c_2)$ and for each pair, we need $O(n)$ time for computing the cost of
the corresponding $2$-clustering. Further, the set $|C|$ has size $O(n \epsilon^{-d} \log (1/\epsilon))$
as shown above. However, we only need to consider an $\epsilon$-complete set of
pairs $P$ which has size $|P|=O(n \epsilon^{-d})=O(n)$. Thus, this implementation
would need to process $O(n)$ pairs at a cost of $O(n)$ each, implying a runtime of
$O(n^2)$ for a fixed $\epsilon$.

Further, using approximate range searching, computing the approximate cost can be done in $O(\log n)$ time (down from $O(n)$). As each preprocessing step takes at most $O(n \log n)$ and the main for loop takes $O(n \log n)$ for a fixed $\epsilon$, the algorithm can thus run in $O(n \log n)$.

\subsection{$(1+\epsilon)$-approximate k-means clustering}

The full k-clustering algorithm builds on the intuition of the 2-clustering version and employs a recursive approach for finding the $(1+\epsilon)$ approximate solution to the full k-means problem.

Before delving into the technical details, intuitively, the algorithm works by employing recursion to test each possible split of
reasonable subsections, specifically, not only computing the best approximate pair for
directly performing a $k$-clustering, but also the approximate cost of a ($k-1$)-clustering.
For this ($k-1$) clustering the algorithm then performs a $2$-clustering on one
of the clusters to produce a full $k$ clustering. This recursive approach is
performed from 2 to $k$. This recursive nature allows testing all the combinatorial possibilities
for how the approximately optimal clustering could be formed. The algorithm mitigates
the computational overhead of this approach by virtue of intelligently determining
the order which the variously sized clusters are computed and continuing the
approximate computation methods used in the $2$-clustering algorithm. By using
this recursive approach, it allows the approximation to be quite rough as it needs only
to hold for one of the possible pairings.

We now switch away from this intuitive explanation and provide the technical
pseudocode for the algorithm below:

\begin{algorithm}[H]
  \caption{(1+$\epsilon$) $k$-means algorithm:}
\begin{algorithmic} 
\IF {$k=1$}
  \STATE return $X$ (as the single cluster) and $cost(X)$
\ENDIF
\FOR {$k^*=2,3,...,k$}
  \STATE generate a set $\cC^*$ of ordered $k^*$-tuples for $C$
  \FORALL {($c_1,c_2,....,c_{k^*}$) $\in$ $\cC^*$}
    \STATE Let ($X_1,X_2,...,X_{k^*}$)=$\Pi_{\text{Vor}}(c_1,c_2,...,c_{k^*})$ be
    the Voronoi partition of $X$, and let $C_i$ be the points of $C$ lying in the
    $\epsilon \omega$-neighborhood of $c_i$, where $\omega$ is the distance of
    the two closest points among $c_1,c_2,...,c_{k^*}$.
    \IF {$C_i = \emptyset$}
      \STATE Disregard the current $k^*$-tuple
      ($c_1,c_2,...,c_{k^*}$)
    \ELSE
      \FOR {$i=1,2,...,k^*$}
        \STATE Call the procedure recursively on $X_i$ and $C_i$, with
        the number of clusters $k_i$ running from 1 to $k-k^*+1$
      \ENDFOR
      \STATE One now has a $k_i$-clustering of $X_i$ for all the
      considered $k_i$.
      \STATE Find the combination
      ($k_1,k_2,...,k_{k^*}$),$k_1+k_2+...+k_{k^*}=k$ for which the
      $k$-clusting of $X$ obtained by combining the comptuer
      $k_i$-clusterings of the $X_i$'s has the smallest cost.
    \ENDIF
  \ENDFOR
\ENDFOR
\STATE For each $k^*$-tuple in $\cC^*$ with all the $C_i$ nonempty,
we know have a single k-clustering. Now return the k-clustering with
the smallest cost (minimum over all $k^*$-tuples and over all $k^*=2,3,...,k$.
\end{algorithmic}
\end{algorithm}

\begin{remark}
The proof of running time for this method builds on the proof for approximate $2$-clustering and employs the recursive nature of the algorithm to go from $O(n \log n)$ for 2-clustering to $O(n (\log n)^k)$ for the $k$-deep recursion tree of the approximate $k$-clustering.
\end{remark}

